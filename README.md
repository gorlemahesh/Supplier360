# Supplier360 â€“ AI + AWS Bedrock Supplier Trust Scoring System

Supplier360 is an end-to-end **AI + AWS Bedrockâ€“powered Supplier 360Â° view** that helps procurement and risk teams:

- Evaluate **supplier compliance** against internal policies  
- Detect **duplicate / overlapping suppliers**  
- Monitor **performance & risk signals** over time  

The core intention of this project is to generate a unified Supplier Trust Score by combining data integrity, certification health, and operational performance. The system uses Amazon Bedrock Agents with Lambda-based action groups and an Aurora PostgreSQL backend, supported by a lightweight Streamlit UI for local demos.

---

## 1. Key Features

- ğŸ¤– **Bedrock Agentâ€“orchestrated workflow**  
  Interact using natural language prompts such as â€œGenerate a supplier risk profileâ€ or  
  â€œWhat is the compliance score for Supplier X?â€. The Agent coordinates all Lambda action groups  
  to produce a unified analysis.

- ğŸ“Š **Compliance scoring engine**  
  Lambda functions query Aurora PostgreSQL to evaluate certification validity, identify missing or  
  expired documents, and generate a structured Certification Score.

- ğŸ§¹ **Supplier deduplication & data integrity check**  
  Fuzzy matching helps detect duplicate or overlapping supplier records, ensuring accurate supplier  
  identity before any scoring is performed.

- ğŸ“ˆ **Operational performance insights**  
  Aggregates delivery accuracy, quality compliance, invoice accuracy, and incident history to  
  compute the Operational Score.

- ğŸ§® **Unified Supplier Trust Score**  
  Data integrity, certification health, and operational performance are combined using a weighted  
  scoring model to produce a final Supplier Trust Score.

- ğŸ–¥ï¸ **Lightweight Streamlit demo UI**  
  A simple chat-style interface for interacting with the Bedrock Agent and exploring supplier  
  insights locally.

---

## 2. High-Level Architecture

**Core flow (for a typical query):**

1. User opens the **Streamlit UI** and asks a question  
   â†’ e.g., â€œWhat is the compliance health report for Supplier A?â€

2. Streamlit calls **Amazon Bedrock Agent Runtime** â†’ `InvokeAgent`.

3. The **Bedrock Agent** decides which **Action Group** to call:
   - `compliance` (compliance health, risk score)
   - `deduplication` (duplicate supplier detection)
   - `performance` (supplier performance metrics)

4. The selected **Lambda function** (one per action group) runs:
   - Uses shared helpers from `backend/common`  
   - Connects to **Aurora PostgreSQL (RDS)**  
   - Runs SQL queries  
   - Computes scores / aggregates  
   - Returns structured JSON back to the Agent

5. The **Agent** formats a natural language response and returns it to the **Streamlit UI**.

> Note: For the demo, the **UI is hosted locally**, while AWS resources (Bedrock, Lambda, Aurora) run in the cloud.

---

## 3. Repository Structure

```text
.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md                # (This file)
â”œâ”€â”€ agent
â”‚   â”œâ”€â”€ action-groups
â”‚   â”‚   â”œâ”€â”€ compliance.yaml  # Bedrock action group definition â€“ compliance
â”‚   â”‚   â”œâ”€â”€ deduplication.yaml
â”‚   â”‚   â””â”€â”€ performance.yaml
â”‚   â””â”€â”€ prompts
â”‚       â””â”€â”€ system_prompt.md # System prompt and instructions for the Agent
â”œâ”€â”€ backend
â”‚   â”œâ”€â”€ common
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ bedrock_utils.py # (If used) Shared utilities for Bedrock / parsing
â”‚   â”‚   â””â”€â”€ rds_client.py    # Helper to connect to Aurora / Postgres
â”‚   â””â”€â”€ lambdas
â”‚       â”œâ”€â”€ compliance
â”‚       â”‚   â””â”€â”€ lambda_function.py
â”‚       â”œâ”€â”€ deduplication
â”‚       â”‚   â””â”€â”€ lambda_function.py
â”‚       â””â”€â”€ performance
â”‚           â””â”€â”€ lambda_function.py
â”œâ”€â”€ db
â”‚   â”œâ”€â”€ data
â”‚   â”‚   â””â”€â”€ data.sql         # Sample / synthetic supplier data
â”‚   â””â”€â”€ schema
â”‚       â””â”€â”€ schema.sql       # Aurora / Postgres DDL (tables, indexes, etc.)
â”œâ”€â”€ docs
â”‚   â””â”€â”€ architecture-overview.md  # High-level design & diagrams
â””â”€â”€ frontend
    â””â”€â”€ streamlit_app
        â”œâ”€â”€ README.md
        â”œâ”€â”€ app.py           # Streamlit UI (chat with Bedrock Agent)
        â””â”€â”€ requirements.txt # Python dependencies for the UI
```
---
## 4. Tech Stack

### ğŸ§  AI & Agent Framework
- **Amazon Bedrock Agents**
- **Bedrock Agent Runtime (InvokeAgent)**
- **YAML Action Groups** mapped to Lambda functions
- **System Prompt** defining agent rules and reasoning

### ğŸ–¥ï¸ Backend (Serverless)
- **AWS Lambda (Python 3.x)** for compliance, deduplication, and performance logic
- **Amazon Aurora PostgreSQL (RDS)** for supplier master data and scoring data
- **VPC + Private Subnets** for secure database connectivity
- **IAM Roles & Policies** enabling:
  - Lambda â†’ RDS connectivity  
  - Bedrock Agent â†’ Lambda invocation  
  - CloudWatch logging  

### ğŸ—„ï¸ Database Layer
- **Aurora PostgreSQL**
- Includes:
  - `db/schema/schema.sql` â€” table definitions and indexes  
  - `db/data/data.sql` â€” synthetic sample dataset for demo  

### ğŸ“¦ Python Shared Utilities (Backend)
Located under `backend/common/`:
- `rds_client.py` â€” PostgreSQL database connector  
- `bedrock_utils.py` â€” common utilities for formatting or Bedrock operations  

### ğŸ–¼ï¸ Frontend (Local Demo)
- **Streamlit** UI (`frontend/streamlit_app/app.py`)
- Integrates with Bedrock Agent Runtime via `boto3`
- Chat-based interface for interacting with the Supplier360 agent

### ğŸ“¦ Frontend Dependencies
---
- streamlit
- boto3
- botocore
- python-dotenv
---

## 5. Prerequisites

Before running or deploying Supplier360, ensure the following setup and tools are available.

### ğŸ” AWS Requirements
- Active **AWS Account**
- **Amazon Bedrock** access enabled (Agents + Runtime)
- **AWS Lambda** service available
- **Amazon Aurora PostgreSQL** cluster created
- IAM permissions allowing:
  - Bedrock Agent â†’ invoke Lambda  
  - Lambda â†’ connect to Aurora RDS  
  - Lambda â†’ write logs to CloudWatch  

### ğŸ—„ï¸ Database Requirements
- Aurora PostgreSQL instance with:
  - Host / Endpoint  
  - Port (default: 5432)  
  - Username & Password  
  - Database name (recommended: `supplier360`)  

- Apply database schema and Load synthetic dataset:
  ```bash
  psql -h <RDS_ENDPOINT> -U <USER> -d <DB_NAME> -f db/schema/schema.sql
  psql -h <RDS_ENDPOINT> -U <USER> -d <DB_NAME> -f db/data/data.sql
  ```
  
### ğŸ Local Machine Requirements:
 -  Python 3.9 or higher
 - Package manager:
 - pip or
 - conda or
 - virtualenv
   
## 6. ğŸš€ Quickstart â€“ Run Supplier360 Locally  

### 1ï¸âƒ£ Clone the repository  
```bash
git clone https://github.com/gorlemahesh/Supplier360.git  
cd Supplier360  
```
### 2ï¸âƒ£ Create and activate a virtual environment  

Using Python venv: 
```bash
python3 -m venv venv  
source venv/bin/activate      # macOS / Linux  
venv\Scripts\activate         # Windows PowerShell 
```
Or using Conda:  
```bash
conda create -n supplier360 python=3.10 -y  
conda activate supplier360  
```
### 3ï¸âƒ£ Install frontend dependencies (Streamlit UI)  
```bash
cd frontend/streamlit_app  
pip install -r requirements.txt
```
### 4ï¸âƒ£ Configure environment variables  
```
cp .env.example .env  
```
Fill in: AWS_REGION, BEDROCK_AGENT_ID, BEDROCK_AGENT_ALIAS_ID, DB_HOST, DB_USER, DB_PASSWORD, etc.  
These will be automatically loaded by the Streamlit app and backend utilities.  
### 5ï¸âƒ£ Run the Streamlit App  
```
streamlit run app.py  
```
Your local demo will be available at: http://localhost:8501  
ğŸ‰ Supplier360 is now ready!


